# -*- coding: utf-8 -*-
"""DL_07_RNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1C4c6ORT7yddwh0W5z6Ho6cizAIif4DyD
"""

# In this mini project, we will deal with time series, and we will learn to
# predict one variable as a function of other variables within the time series.
# We will use Recurrent Neural Networks (RNN's) to do so.

# Example takend and adapted from:
# https://towardsdatascience.com/predicting-stock-price-with-lstm-13af86a74944
# Data from: https://www.kaggle.com/darkknight91/ge-stock
# 55 years of stock price data for GE for US

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler

"""# Parte 1: Carga y limpia los datos"""

# Requiered to select a file to be imported into colab
from google.colab import files
uploaded = files.upload()

# Load file from colab (G drive)
df = pd.read_csv('ge.us.txt')
df.dropna(inplace=True)
df

# Checking for NaN values.
print("checking if any null values are present\n", df.isna().sum())

# Plot info
plt.figure(figsize=(18, 4))
plt.subplot(1, 2, 1)
plt.plot(df["Open"])
plt.plot(df["High"])
plt.plot(df["Low"])
plt.plot(df["Close"])
plt.title('GE stock price history')
plt.ylabel('Price (USD)')
plt.xlabel('Days')
plt.legend(['Open','High','Low','Close'], loc='upper left')
plt.grid()

plt.subplot(1, 2, 2)
plt.plot(df["Volume"])
plt.title('GE stock volume history')
plt.ylabel('Volume')
plt.xlabel('Days')
plt.grid()
plt.show()

# Split train and test sets
df_train, df_test = train_test_split(df, train_size=0.8, test_size=0.2, shuffle=False)
print("Train and Test size", len(df_train), "--", len(df_test))
# Still in DataFrame format
# Notice the use of shuffle=False

# Scale features and put them in matrix format (DataFrame --> Matrix)
useful_cols = ["Open", "High", "Low", "Close", "Volume"]

min_max_scaler = MinMaxScaler()
train_data = min_max_scaler.fit_transform(df_train.loc[:, useful_cols].values)
test_data = min_max_scaler.transform(df_test.loc[:, useful_cols].values)

# Here we have two sets. One with 11246 timesteps, each of 5 features, and other
# with 2812 timesteps, each of 5 features
print(train_data.shape)
print(test_data.shape)

# Plot time series
plt.figure(figsize=(16, 6))

plt.subplot(2, 2, 1)
plt.plot(train_data[:, :-1])
plt.title('Training set')
plt.ylabel('Price (USD)')
plt.xlabel('Days')
plt.legend(['Open','High','Low','Close'], loc='upper left')
plt.grid()

plt.subplot(2, 2, 2)
plt.plot(test_data[:, :-1])
plt.title('Test set')
plt.ylabel('Price (USD)')
plt.xlabel('Days')
plt.legend(['Open','High','Low','Close'], loc='upper left')
plt.grid()

plt.subplot(2, 2, 3)
plt.plot(train_data[:, -1])
plt.ylabel('Volume')
plt.xlabel('Days')
plt.legend(['Volume'], loc='upper left')
plt.grid()

plt.subplot(2, 2, 4)
plt.plot(test_data[:, -1])
plt.ylabel('Volume')
plt.xlabel('Days')
plt.legend(['Volume'], loc='upper left')
plt.grid()

plt.show()

# Plot distribution of variables
plt.figure(figsize=(16, 6))

plt.subplot(2, 2, 1)
plt.boxplot(train_data[:, :-1])
plt.title('Training set')
plt.ylabel('Price (USD)')
plt.xticks(ticks=range(1, 5), labels=['Open','High','Low','Close'])
plt.grid()

plt.subplot(2, 2, 2)
plt.boxplot(test_data[:, :-1])
plt.title('Test set')
plt.ylabel('Price (USD)')
plt.xticks(ticks=range(1, 5), labels=['Open','High','Low','Close'])
plt.grid()

plt.subplot(2, 2, 3)
plt.boxplot(train_data[:, -1])
plt.ylabel('Volume')
plt.xticks(ticks=range(1, 2), labels=['Volume'])
plt.grid()

plt.subplot(2, 2, 4)
plt.boxplot(test_data[:, -1])
plt.ylabel('Volume')
plt.xticks(ticks=range(1, 2), labels=['Volume'])
plt.grid()

plt.show()

"""# Parte 2: Formatea los datos para procesarlos con RNNs"""

# GOAL:
# Predict 'close' value, from 4 historic timesteps,
# y[t] = f(x[t-1], x[t-2], x[t-3], x[t-4]; \Omega),
# where x[t] = [open, high, low, close, volume][t],
# and y[t] = [close][t]

# AUX funtion to build time-series dataset
def build_dataset(X, Y, hist_size):
  '''
  Params
    X: data matrix [n_time_steps, n_X_feats]
    Y: label matrix [n_time_steps, n_Y_feats]
    hist_size: integer indicating the number of time steps in each sample of x
  Returns
    x: tensor of input data [n_samples, n_timesteps, n_X_features]
    y: tensor of output data [n_samples, n_Y_features]
  '''
  n_samples = Y.shape[0] - hist_size
  x = np.zeros((n_samples, hist_size, X.shape[1]))
  y = np.zeros((n_samples))

  for ind in range(n_samples):    
    x[ind] = X[ind : ind + hist_size]
    y[ind] = Y[ind + hist_size]
  return x, y

# Create time-series datasets: Use all features as input, and 'close' as output.
time_steps = 4
x_train, y_train = build_dataset(train_data, train_data[:, 3], time_steps)
x_test,  y_test  = build_dataset(test_data,  test_data[:, 3],  time_steps)

print(x_train.shape)
print(x_test.shape)
print(y_train.shape)
print(y_test.shape)

# Print some examples
print("Original first 10 elements")
print(train_data[:10])
print("\nTime-series format of first 4 elements x")
print(x_train[:4])
print("\nTime-series format of first 4 elements y")
print(y_train[:4])

"""# Parte 3: Crea la RNN y entrenala"""

from tensorflow.keras.models import Sequential
  from tensorflow.keras.layers import Input, Dropout, SimpleRNN, LSTM, Dense, LeakyReLU
  from tensorflow.keras.utils import plot_model

# Create RNN model
rnn_model = Sequential()
rnn_model.add(Input(shape=(x_train.shape[1], x_train.shape[2])))
rnn_model.add(SimpleRNN(units=32))
rnn_model.add(Dense(1, activation='sigmoid'))

rnn_model.summary()

# Compile
rnn_model.compile(loss='mean_squared_error', optimizer='rmsprop')

# Plot model
plot_model(rnn_model, show_shapes=True, show_dtype=True, show_layer_names=True,
           dpi=48, show_layer_activations=True)

# == Q1 ==
# ¿Qué activaciones no lineales usa RNN por defecto?
# ¿Qué activaciones no lineales usa LSTM por defecto?
# ¿Por qué uso la activación 'sigmoide' a la salida de la red?

# Weights in the first layer (simpleRNN, Input does not count as layer)
B, A, C = rnn_model.layers[0].get_weights()
print(Wx.shape)
print(Wh.shape)
print(b.shape)

# == Q2 == 
# Renombra las variables A, B, C, usando la notación vista en clase: W(h), W(x), etc...

# Weights in the second layer (FullyConnected)
W, b = rnn_model.layers[1].get_weights()
print(W.shape)
print(b.shape)

# Train
rnn_model.fit(x_train, y_train, batch_size=32, epochs=10, validation_split=0.1)

# Plot loss
plt.figure(figsize=(12, 4))
plt.plot(rnn_model.history.history['loss'], label='Training', linewidth=2)
plt.plot(rnn_model.history.history['val_loss'], label='Validation', linewidth=2)
plt.legend()
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.grid(True)
plt.show()

# Predict on training and test sets
y_train_hat = rnn_model.predict(x_train)
y_test_hat = rnn_model.predict(x_test)
print(y_train_hat.shape)
print(y_test_hat.shape)

# Plot y_train, y_train_hat, y_test and y_test_hat
x_ticks = np.arange(len(y_train) + len(y_test)) # Para poder imprimir test delante de train

plt.figure(figsize=(12, 4))
plt.plot(x_ticks[:len(y_train)], y_train, linewidth=3)
plt.plot(x_ticks[:len(y_train)], y_train_hat, linewidth=1)
plt.plot(x_ticks[len(y_train):], y_test, linewidth=3)
plt.plot(x_ticks[len(y_train):], y_test_hat, linewidth=1)
plt.ylabel('Close value')
plt.xlabel('Days')
plt.legend(['y_train', '$\hat{y}$_train', 'y_test', '$\hat{y}$_test'], loc='upper left')
plt.grid()
plt.show()

# Let's go back to the original scale
# == Q3 ==
# ¿Qué está sucediendo en esta celda?
y_train_back = y_train * min_max_scaler.data_range_[3] + min_max_scaler.data_min_[3]
y_test_back = y_test * min_max_scaler.data_range_[3] + min_max_scaler.data_min_[3]

y_train_hat_back = y_train_hat * min_max_scaler.data_range_[3] + min_max_scaler.data_min_[3]
y_test_hat_back = y_test_hat * min_max_scaler.data_range_[3] + min_max_scaler.data_min_[3]

# Plot y_train, y_train_hat, y_test and y_test_hat
x_ticks = np.arange(len(y_train) + len(y_test)) # Para poder imprimir test delante de train

plt.figure(figsize=(12, 4))
plt.plot(x_ticks[:len(y_train)], y_train_back, linewidth=3)
plt.plot(x_ticks[:len(y_train)], y_train_hat_back, linewidth=1)
plt.plot(x_ticks[len(y_train):], y_test_back, linewidth=3)
plt.plot(x_ticks[len(y_train):], y_test_hat_back, linewidth=1)
plt.ylabel('Close value')
plt.xlabel('Days')
plt.legend(['y_train', '$\hat{y}$_train', 'y_test', '$\hat{y}$_test'], loc='upper left')
plt.grid()
plt.show()

# == Q4 ==
# Revisa la documentación de tensorflow para las capas simpleRNN y LSTM.
# ¿Qué sucede definimos el parámetro return_sequence=True?
# ¿Para qué sirve el parámetro return_state?

"""# Parte 4: Predice con "delay""""

# Crea una función "delayed_dataset" que formatee los datos para asociar y[t] con
# x[t-delay] hasta x[t-delay-history]. Por ejemplo, para hacer prónosticos del tipo
# y[10] = f(x[7], x[6], x[5]; \Omega). Donde tanto x como y pueden ser univariados
# o multivariados.
#
# For instance, calling such a function with the 
# x_train, y_train = delayed_dataset(X_train[:20, :3], X_train[:20, 3:], delay=2, hist=4)
# must return x_train and y_train with the following shapes,
# x_train.shape = (14, 4, 3) # 14 samples, each of 4 time-steps, each of 3 features
# y_train.shape = (14, 2)    # 14 label-samples, each of two values
#
# The following table shows indices information (no need to print it)
# IND:  0, Xids:  0 --  3, -- Yids:  5
# IND:  1, Xids:  1 --  4, -- Yids:  6
# IND:  2, Xids:  2 --  5, -- Yids:  7
# IND:  3, Xids:  3 --  6, -- Yids:  8
# IND:  4, Xids:  4 --  7, -- Yids:  9
# IND:  5, Xids:  5 --  8, -- Yids: 10
# IND:  6, Xids:  6 --  9, -- Yids: 11
# IND:  7, Xids:  7 -- 10, -- Yids: 12
# IND:  8, Xids:  8 -- 11, -- Yids: 13
# IND:  9, Xids:  9 -- 12, -- Yids: 14
# IND: 10, Xids: 10 -- 13, -- Yids: 15
# IND: 11, Xids: 11 -- 14, -- Yids: 16
# IND: 12, Xids: 12 -- 15, -- Yids: 17
# IND: 13, Xids: 13 -- 16, -- Yids: 18

# AUX funtion to build time-series dataset
def delayed_dataset(X, Y, delay, hist_size):
  '''
  Params
    X: data matrix [n_time_steps, n_X_feats]
    Y: label matrix [n_time_steps, n_Y_feats]
    delay: integer indicating the delay between last time step in x and time step in y
    hist_size: integer indicating the number of time steps in each sample of x
  Returns
    x: tensor of input data [n_samples, n_timesteps, n_X_features]
    y: tensor of output data [n_samples, n_Y_features]
  '''
  n_samples = Y.shape[0] - (delay + hist_size)
  x = np.zeros((n_samples, hist_size, X.shape[1]))
  y = np.zeros((n_samples, Y.shape[1]))

  for ind in range(n_samples):    
    x[ind] = X[ind : ind + hist_size]
    y[ind] = Y[ind + hist_size + delay]
  return x, y

# Usa la función anterior para generar sets de entrenamiento y validación donde,
# 'x' contenga (open, high, low), e 'y' contenga (close, volume)
# Considera delay=3 y history_size=4
x_train, y_train = delayed_dataset(train_data[:, :3], train_data[:, 3:], 3, 4)
x_test, y_test   = delayed_dataset(test_data[:, :3],  test_data[:, 3:],  3, 4)

print(x_train.shape)
print(x_test.shape)
print(y_train.shape)
print(y_test.shape)

# Let us remember the range of the target variables
plt.subplot(1, 2, 1)
plt.boxplot(y_train)
plt.subplot(1, 2, 2)
plt.boxplot(y_test)
plt.show()

# == Q5 ==
# Diseña una RNN para resolver el problema de regresion multivariada que acabas de generar.
# Reporta el mejor resultado que hayas obtenido.

# Your RNN Here
rnn_model = Sequential()
rnn_model.add(Input(shape=(x_train.shape[1], x_train.shape[2])))
rnn_model.add(LSTM(units=32))
rnn_model.add(Dense(16))
rnn_model.add(LeakyReLU())
rnn_model.add(Dense(2))

rnn_model.summary()

# Compile
rnn_model.compile(loss='mean_squared_error', optimizer='rmsprop')

# Train
history = rnn_model.fit(x_train, y_train, epochs=30, batch_size=4, validation_split=0.1)

# Plot loss
plt.figure(figsize=(12, 4))
plt.plot(history.history['loss'], label='Training', linewidth=2)
plt.plot(history.history['val_loss'], label='Validation', linewidth=2)
plt.legend()
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.grid(True)
plt.show()

# Predict on training and test sets
y_train_hat = rnn_model.predict(x_train)
y_test_hat = rnn_model.predict(x_test)
print(y_train_hat.shape)
print(y_test_hat.shape)

# Plot y_train, y_train_hat, y_test and y_test_hat
x_ticks = np.arange(len(y_train) + len(y_test))

plt.figure(figsize=(18, 4))
plt.subplot(1, 2, 1)
plt.plot(x_ticks[:len(y_train)], y_train[:, 0])
plt.plot(x_ticks[:len(y_train)], y_train_hat[:, 0])
plt.plot(x_ticks[len(y_train):], y_test[:, 0])
plt.plot(x_ticks[len(y_train):], y_test_hat[:, 0])
plt.ylabel('Close value')
plt.xlabel('Days')
plt.legend(['y_train', '$\hat{y}$_train', 'y_test', '$\hat{y}$_test'], loc='upper left')
plt.grid()

plt.subplot(1, 2, 2)
plt.plot(x_ticks[:len(y_train)], y_train[:, 1])
plt.plot(x_ticks[:len(y_train)], y_train_hat[:, 1])
plt.plot(x_ticks[len(y_train):], y_test[:, 1])
plt.plot(x_ticks[len(y_train):], y_test_hat[:, 1])
plt.ylabel('Volume value')
plt.xlabel('Days')
plt.legend(['y_train', '$\hat{y}$_train', 'y_test', '$\hat{y}$_test'], loc='upper left')
plt.grid()

plt.show()

# == Q6 ==
# Parece que predecir 'close' es muy fácil, pero no predecir 'volume'.
# Explica las razones por las cuales crees que obtienes esos resultados.

# == Q7 ==
# Ahora encuentra la forma de pasar columnas no consecutivas a la función "delayed_dataset".
# Crea sets donde,
# 'x' contenga (open, low, volume), e 'y' contenga (high, close)
# Considera delay=3 y history_size=4
x_train, y_train = delayed_dataset(np.vstack((train_data[:, 0], train_data[:, 2], train_data[:, 4])).T,
                                   np.vstack((train_data[:, 1], train_data[:, 3])).T, 3, 4)
x_test , y_test  = delayed_dataset(np.vstack((test_data[:, 0] , test_data[:, 2] , test_data[:, 4] )).T,
                                   np.vstack((test_data[:, 1] , test_data[:, 3] )).T, 3, 4)

print(x_train.shape)
print(x_test.shape)
print(y_train.shape)
print(y_test.shape)

# == Q8 ==
# Diseña una red para resolver este último problema de regresión.
# Reporta tu red con el mejor resultado.

# Your RNN Here
rnn_model = Sequential()
rnn_model.add(Input(shape=(x_train.shape[1], x_train.shape[2])))

rnn_model.summary()

# Compile
rnn_model.compile(loss='mean_squared_error', optimizer='rmsprop')

# Train
history = rnn_model.fit(x_train, y_train, epochs=32, verbose=1, batch_size=4, validation_split=0.1)

# Plot loss
plt.figure(figsize=(12, 4))
plt.plot(history.history['loss'], label='Training', linewidth=2)
plt.plot(history.history['val_loss'], label='Validation', linewidth=2)
plt.legend()
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.grid(True)
plt.show()

# Predict on training and test sets
y_train_hat = rnn_model.predict(x_train)
y_test_hat = rnn_model.predict(x_test)
print(y_train_hat.shape)
print(y_test_hat.shape)

# Plot y_train, y_train_hat, y_test and y_test_hat
x_ticks = np.arange(len(y_train) + len(y_test))

plt.figure(figsize=(18, 4))
plt.subplot(1, 2, 1)
plt.plot(x_ticks[:len(y_train)], y_train[:, 0])
plt.plot(x_ticks[:len(y_train)], y_train_hat[:, 0])
plt.plot(x_ticks[len(y_train):], y_test[:, 0])
plt.plot(x_ticks[len(y_train):], y_test_hat[:, 0])
plt.ylabel('High value')
plt.xlabel('Days')
plt.legend(['y_train', '$\hat{y}$_train', 'y_test', '$\hat{y}$_test'], loc='upper left')
plt.grid()

plt.subplot(1, 2, 2)
plt.plot(x_ticks[:len(y_train)], y_train[:, 1])
plt.plot(x_ticks[:len(y_train)], y_train_hat[:, 1])
plt.plot(x_ticks[len(y_train):], y_test[:, 1])
plt.plot(x_ticks[len(y_train):], y_test_hat[:, 1])
plt.ylabel('Close value')
plt.xlabel('Days')
plt.legend(['y_train', '$\hat{y}$_train', 'y_test', '$\hat{y}$_test'], loc='upper left')
plt.grid()

plt.show()
# NOTA: las gráficas son muy parecidas, pero no iguales. -- El desempeño es bueno.

